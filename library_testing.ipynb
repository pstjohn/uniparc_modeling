{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='talk', style='ticks',\n",
    "        color_codes=True, rc={'legend.frameon': False})\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8000\n",
    "max_seq_len = 512\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('sentencepiece_models/uniparc_10M_8000.model')\n",
    "\n",
    "def sp_encode(line_tensor):\n",
    "    encoded_array = np.asarray(\n",
    "        sp.SampleEncodeAsIds(line_tensor.numpy(), nbest_size=-1, alpha=0.5))\n",
    "\n",
    "    # If the protein sequence is too long, take a random slice.\n",
    "    if len(encoded_array) > max_seq_len:\n",
    "        offset = np.random.randint(\n",
    "            low=0, high=len(encoded_array) - max_seq_len + 1)\n",
    "        encoded_array = encoded_array[offset:(offset + max_seq_len)]\n",
    "\n",
    "    return encoded_array\n",
    "\n",
    "def sp_encode_tf(line_tensor):\n",
    "    return tf.py_function(sp_encode, inp=[line_tensor], Tout=[tf.int32,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 512), (None, 512)), (None, 512, 1)), types: ((tf.int32, tf.bool), tf.int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert.dataset import create_masked_input_dataset\n",
    "\n",
    "training_data = create_masked_input_dataset(\n",
    "    encode_fn=sp_encode_tf,\n",
    "    sequence_path='/projects/bpms/pstjohn/uniparc/sequences_train.txt',\n",
    "    max_sequence_length=max_seq_len,\n",
    "    batch_size=10,\n",
    "    buffer_size=1024,\n",
    "    vocab_size=vocab_size,\n",
    "    mask_index=4,\n",
    "    vocab_start=5,\n",
    "    fix_sequence_length=True)\n",
    "\n",
    "training_data.repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_data = create_masked_input_dataset(\n",
    "    encode_fn=sp_encode_tf,\n",
    "    sequence_path='/projects/bpms/pstjohn/uniparc/sequences_valid.txt',\n",
    "    max_sequence_length=max_seq_len,\n",
    "    batch_size=10,\n",
    "    buffer_size=1024,\n",
    "    vocab_size=vocab_size,\n",
    "    mask_index=4,\n",
    "    vocab_start=5,\n",
    "    fix_sequence_length=True)\n",
    "\n",
    "valid_data.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked_seqs, input_masks), true_values = next(iter(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7982"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_seqs.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 512, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT layers development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_embedding_3 (TokenEmbeddi multiple             256000      input_7[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "position_embedding_3 (PositionE (None, 512, 32)      16416       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 512, 32)      0           token_embedding_3[0][0]          \n",
      "                                                                 position_embedding_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "projection_3 (Projection)       (None, 512, 64)      2240        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "transformer_3 (Transformer)     (None, 512, 64)      49984       projection_3[0][0]               \n",
      "                                                                 transformer_3[0][0]              \n",
      "                                                                 transformer_3[1][0]              \n",
      "                                                                 transformer_3[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512, 32)      2080        transformer_3[3][0]              \n",
      "==================================================================================================\n",
      "Total params: 326,720\n",
      "Trainable params: 326,720\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from bert.layers import (PositionEmbedding, Attention, Transformer, TokenEmbedding, Bias,\n",
    "                         Projection, gelu, masked_sparse_cross_entropy_loss, InverseSquareRootSchedule,\n",
    "                         initializer)\n",
    "\n",
    "vocab_size = 8000\n",
    "max_seq_len = 512\n",
    "embedding_dimension = 32\n",
    "model_dimension = 64\n",
    "transformer_ff_dimension = model_dimension * 4\n",
    "num_attention_heads = model_dimension // 16\n",
    "num_transformer_layers = 4\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "learning_rate = 1E-4\n",
    "warmup_updates = 3000\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(max_seq_len,), dtype=tf.int32, batch_size=None)\n",
    "# input_mask = layers.Input(shape=(max_seq_len,), dtype=tf.bool, batch_size=None)\n",
    "\n",
    "token_embedding_layer = TokenEmbedding(\n",
    "    vocab_size, embedding_dimension, embeddings_initializer=initializer(), mask_zero=True)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "position_embeddings = PositionEmbedding(\n",
    "    max_seq_len + 1, embedding_dimension, embeddings_initializer=initializer(),\n",
    "    mask_zero=True)(inputs)\n",
    "\n",
    "embeddings = layers.Add()([token_embeddings, position_embeddings])\n",
    "embeddings = Projection(model_dimension, use_residual=False)(embeddings)\n",
    "\n",
    "transformer = Transformer(num_attention_heads, transformer_ff_dimension, dropout=dropout_rate)\n",
    "for i in range(num_transformer_layers):\n",
    "    embeddings = transformer(embeddings)\n",
    "\n",
    "out = layers.Dense(embedding_dimension, activation=gelu, kernel_initializer=initializer())(embeddings)\n",
    "out = token_embedding_layer(out, transpose=True)\n",
    "# out = Bias()([out, input_mask])\n",
    "# out = layers.Softmax()(out)\n",
    "\n",
    "model = tf.keras.Model([inputs], [out], name='model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = model([masked_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tf.boolean_mask(inputs, input_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=390444, shape=(183, 8000), dtype=float32, numpy=\n",
       "array([[-0.012174  , -0.01447492,  0.00452682, ..., -0.0060373 ,\n",
       "         0.00932042, -0.01318178],\n",
       "       [-0.0010614 , -0.00480176,  0.01051812, ..., -0.00745522,\n",
       "        -0.00676224, -0.00630224],\n",
       "       [ 0.01036556, -0.00018116,  0.00781351, ..., -0.0041775 ,\n",
       "        -0.01215548, -0.00032609],\n",
       "       ...,\n",
       "       [-0.01486421, -0.0134879 , -0.00079376, ..., -0.00024359,\n",
       "         0.00672316, -0.00470749],\n",
       "       [ 0.0071736 , -0.00071082,  0.00481169, ..., -0.01370568,\n",
       "         0.0018601 , -0.00494374],\n",
       "       [ 0.01894609,  0.00136959,  0.01398921, ..., -0.01621576,\n",
       "        -0.01224323,  0.00046754]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# Horovod: add Horovod DistributedOptimizer.\n",
    "opt = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'],\n",
    "    optimizer=opt)\n",
    "\n",
    "# model.compile(\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#         from_logits=True, reduction=tf.keras.losses.Reduction.NONE),\n",
    "#     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "#     optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 100 steps, validate for 10 steps\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pstjohn/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/pstjohn/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 16s 159ms/step - loss: 0.4173 - accuracy: 0.0171 - val_loss: 0.3895 - val_accuracy: 0.0147\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.3968 - accuracy: 0.0213 - val_loss: 0.3954 - val_accuracy: 0.0181\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.3692 - accuracy: 0.0210 - val_loss: 0.3502 - val_accuracy: 0.0177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f15f1c4d990>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_data, steps_per_epoch=100, epochs=3, verbose=1,\n",
    "          validation_data=valid_data, validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pstjohn/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    'test_model.h5',\n",
    "    custom_objects={\n",
    "        'PositionEmbedding': PositionEmbedding,\n",
    "        'TokenEmbedding': TokenEmbedding,\n",
    "        'Attention': Attention,\n",
    "        'Transformer': Transformer,\n",
    "        'Projection': Projection,        \n",
    "        'Bias': Bias,\n",
    "        'gelu': gelu,\n",
    "    })\n",
    "\n",
    "# true_labels = layers.Input(shape=(None,), dtype=tf.int32, batch_size=None)\n",
    "# model.compile(loss=masked_sparse_cross_entropy_loss, target_tensors=true_labels,\n",
    "#               optimizer=tfa.optimizers.AdamW(weight_decay=0.01, learning_rate=1E-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training_data, steps_per_epoch=100, epochs=3, verbose=1,\n",
    "          validation_data=valid_data, validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_data = valid_data.map(sp_encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE).padded_batch(60, padded_shapes=([512],))\n",
    "# eval_encoded = next(iter(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predict = model.predict(encoded_data.take(3), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predict.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
