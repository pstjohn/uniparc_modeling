{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='talk', style='ticks',\n",
    "        color_codes=True, rc={'legend.frameon': False})\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.dataset import create_masked_input_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_path='uniparc_5M.model'\n",
    "sequence_path='/projects/bpms/pstjohn/uniparc/sequences_train_no_BZUX.txt'\n",
    "max_sequence_length=512\n",
    "batch_size=20\n",
    "buffer_size=1024\n",
    "vocab_size=32000\n",
    "mask_index=4\n",
    "vocab_start=5\n",
    "fix_sequence_length=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(language_model_path)\n",
    "\n",
    "def sp_encode(line_tensor):\n",
    "    encoded_array = np.asarray(\n",
    "        sp.SampleEncodeAsIds(line_tensor.numpy(), nbest_size=-1, alpha=0.5))\n",
    "\n",
    "    # If the protein sequence is too long, take a random slice.\n",
    "    if len(encoded_array) > max_sequence_length:\n",
    "        offset = np.random.randint(\n",
    "            low=0, high=len(encoded_array) - max_sequence_length + 1)\n",
    "        encoded_array = encoded_array[offset:(offset + max_sequence_length)]\n",
    "\n",
    "    return encoded_array\n",
    "\n",
    "def sp_decode(line_tensor):\n",
    "    return sp.DecodeIds(line_tensor.numpy().tolist())\n",
    "\n",
    "def sp_encode_tf(line_tensor):\n",
    "    return tf.py_function(sp_encode, inp=[line_tensor], Tout=[tf.int32,])\n",
    "\n",
    "def mask_input(input_tensor):\n",
    "    \"\"\" Randomly mask the input tensor according to the formula perscribed by BERT. \n",
    "    Randomly masks 15% of input tokens, with 80% recieving the [MASK] token,\n",
    "    10% randomized, 10% left unchanged. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    masked_tensor: (batch_size, seq_length) \n",
    "        Tensor with masked values\n",
    "    input_tensor: (batch_size, seq_length)\n",
    "        Original input tensor (true values)\n",
    "    input_mask: (batch_size, seq_length)\n",
    "        Boolean mask that selects the desired inputs.    \n",
    "    \"\"\"\n",
    "\n",
    "    mask_score = tf.random.uniform(input_tensor.shape, maxval=1, dtype=tf.float32)\n",
    "    input_mask = mask_score < .15\n",
    "\n",
    "    # Mask with [MASK] token 80% of the time\n",
    "    mask_mask = mask_score <= 0.15 * 0.8 \n",
    "\n",
    "    # Mask with random token 10% of the time\n",
    "    mask_random = (mask_score >= 0.15 * 0.9) & input_mask\n",
    "\n",
    "    # Tensors to replace with where input is masked or randomized\n",
    "    mask_value_tensor = tf.ones(input_tensor.shape, dtype=tf.int32) * mask_index\n",
    "    random_value_tensor = tf.random.uniform(\n",
    "        input_tensor.shape, minval=vocab_start, maxval=vocab_size, dtype=tf.int32)\n",
    "    pad_value_tensor = tf.zeros(input_tensor.shape, dtype=tf.int32)\n",
    "\n",
    "    # Use the replacements to mask the input tensor\n",
    "    masked_tensor = tf.where(mask_mask, mask_value_tensor, input_tensor)\n",
    "    masked_tensor = tf.where(mask_random, random_value_tensor, masked_tensor)\n",
    "\n",
    "    # Set true values to zero (pad value) where not masked\n",
    "    true_tensor = tf.where(input_mask, input_tensor, pad_value_tensor)\n",
    "\n",
    "    return masked_tensor, input_mask, true_tensor\n",
    "\n",
    "def mask_input_tf(input_tensor):\n",
    "    a, b, c = tf.py_function(mask_input, inp=[input_tensor], Tout=[tf.int32, tf.bool, tf.int32])\n",
    "    return (a, b), c\n",
    "\n",
    "\n",
    "valid_data = tf.data.TextLineDataset(sequence_path)\n",
    "\n",
    "encoded_data = valid_data\\\n",
    "    .map(sp_encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "    .map(mask_input_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "# This argument controls whether to fix the size of the sequences\n",
    "tf_seq_len = -1 if not fix_sequence_length else max_sequence_length\n",
    "\n",
    "encoded_data = encoded_data\\\n",
    "    .shuffle(buffer_size=buffer_size)\\\n",
    "    .padded_batch(batch_size, padded_shapes=(([tf_seq_len], [tf_seq_len]), [tf_seq_len]))\n",
    "\n",
    "print(next(iter(encoded_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked_seqs, input_mask), true_values = next(iter(encoded_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT layers development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.layers import (Attention, Transformer, TokenEmbedding, Bias,\n",
    "                         gelu, masked_sparse_cross_entropy_loss, BERTLearningRateScheduler)\n",
    "\n",
    "MAX_ENCODED_LENGTH = 512\n",
    "\n",
    "\n",
    "class PositionEmbedding(layers.Embedding):\n",
    "    \"\"\" Return masked embeddings according to the position index of each input\n",
    "    \"\"\"\n",
    "        \n",
    "    def call(self, inputs):\n",
    "\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = inputs.shape[1]\n",
    "\n",
    "        idx = tf.tile(tf.expand_dims(tf.range(seq_len), 0), [batch_size, 1]) + 1\n",
    "        return super(PositionEmbedding, self).call(idx)\n",
    "\n",
    "\n",
    "\n",
    "p_embedding = PositionEmbedding(MAX_ENCODED_LENGTH + 1, MAX_ENCODED_LENGTH, mask_zero=True)\n",
    "assert np.all(p_embedding(masked_seqs)[0, 2, :] == p_embedding(masked_seqs)[5, 2, :])\n",
    "\n",
    "inputs = p_embedding(masked_seqs)\n",
    "out = Attention(8, name='test')(inputs)\n",
    "out.shape\n",
    "\n",
    "transformer = Transformer(8, 0.1, name='test')\n",
    "out = transformer(inputs)\n",
    "out2 = transformer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding_layer = TokenEmbedding(\n",
    "    vocab_size, embedding_dimension, embeddings_initializer=initializer(), mask_zero=True)\n",
    "token_embeddings = token_embedding_layer(masked_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embeddings = PositionEmbedding(\n",
    "    MAX_ENCODED_LENGTH + 1, embedding_dimension, embeddings_initializer=initializer(),\n",
    "    mask_zero=True)(masked_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = layers.Add()([token_embeddings, position_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(embeddings._keras_mask == token_embeddings._keras_mask).numpy().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class Bias(layers.Layer):\n",
    "    \"\"\" Final bias layer added to logits prior to softmax scoring. This layer\n",
    "    also clears the _keras_mask attribute from the Transformers in order to\n",
    "    allow the custom loss function to work properly. \"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bias = self.add_weight(name='classifier_bias',\n",
    "                                    dtype=K.floatx(),\n",
    "                                    shape=[input_shape[0][-1]],\n",
    "                                    initializer=tf.zeros_initializer())\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        logits = tf.nn.bias_add(inputs[0], self.bias)\n",
    "        return logits\n",
    "        \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "from bert.layers import initializer\n",
    "\n",
    "embedding_dimension = 4\n",
    "vocab_size = 32000\n",
    "model_dimension = 16\n",
    "num_transformer_layers = 2\n",
    "\n",
    "\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "# with mirrored_strategy.scope():\n",
    "\n",
    "inputs = layers.Input(shape=(MAX_ENCODED_LENGTH,), dtype=tf.int32, batch_size=None)\n",
    "input_mask = layers.Input(shape=(MAX_ENCODED_LENGTH,), dtype=tf.bool, batch_size=None)\n",
    "\n",
    "token_embedding_layer = TokenEmbedding(\n",
    "    vocab_size, embedding_dimension, embeddings_initializer=initializer(), mask_zero=True)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "position_embeddings = PositionEmbedding(\n",
    "    MAX_ENCODED_LENGTH + 1, embedding_dimension, embeddings_initializer=initializer(),\n",
    "    mask_zero=True)(inputs)\n",
    "\n",
    "embeddings = layers.Add()([token_embeddings, position_embeddings])\n",
    "embeddings = layers.Dense(model_dimension)(embeddings)\n",
    "\n",
    "transformer = Transformer(4)\n",
    "for i in range(num_transformer_layers):\n",
    "    embeddings = transformer(embeddings)\n",
    "\n",
    "out = layers.Dense(embedding_dimension, activation=gelu, kernel_initializer=initializer())(embeddings)\n",
    "out = token_embedding_layer(out, transpose=True)\n",
    "out = Bias()([out, input_mask])\n",
    "\n",
    "model = tf.keras.Model([inputs, input_mask], [out], name='model')\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    optimizer=tfa.optimizers.AdamW(weight_decay=0.01, learning_rate=1E-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(encoded_data.prefetch(tf.data.experimental.AUTOTUNE).repeat(), steps_per_epoch=1000, verbose=1, \n",
    "                    callbacks=[BERTLearningRateScheduler(initial_learning_rate=1E-3, num_warmup_steps=10000)], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    'test_model.h5',\n",
    "    custom_objects={\n",
    "        'PositionEmbedding': PositionEmbedding,\n",
    "        'TokenEmbedding': TokenEmbedding,\n",
    "        'Attention': Attention,\n",
    "        'Transformer': Transformer,\n",
    "        'Bias': Bias,\n",
    "        'gelu': gelu,\n",
    "    }, compile=False)\n",
    "\n",
    "true_labels = layers.Input(shape=(None,), dtype=tf.int32, batch_size=None)\n",
    "model.compile(loss=masked_sparse_cross_entropy_loss, target_tensors=true_labels,\n",
    "              optimizer=tfa.optimizers.AdamW(weight_decay=0.01, learning_rate=1E-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_data = valid_data.map(sp_encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE).padded_batch(60, padded_shapes=([512],))\n",
    "# eval_encoded = next(iter(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predict = model.predict(encoded_data.take(3), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predict.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
