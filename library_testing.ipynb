{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='talk', style='ticks',\n",
    "        color_codes=True, rc={'legend.frameon': False})\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.dataset import create_masked_input_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_path='uniparc_5M.model'\n",
    "sequence_path='sequences_train_subset.txt'\n",
    "max_sequence_length=512\n",
    "batch_size=20\n",
    "buffer_size=1024\n",
    "vocab_size=32000\n",
    "mask_index=4\n",
    "vocab_start=5\n",
    "fix_sequence_length=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: id=35051, shape=(20, 512), dtype=int32, numpy=\n",
      "array([[ 6206,   151,  2087, ...,     0,     0,     0],\n",
      "       [  243,   158,  2257, ...,     0,     0,     0],\n",
      "       [  243,     4,     6, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 6206, 21019, 21502, ...,     0,     0,     0],\n",
      "       [    4,     5,  5035, ...,     0,     0,     0],\n",
      "       [  328,    14,   104, ...,     0,     0,     0]], dtype=int32)>, <tf.Tensor: id=35052, shape=(20, 512), dtype=bool, numpy=\n",
      "array([[False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False,  True, False, ..., False, False, False],\n",
      "       ...,\n",
      "       [ True, False, False, ..., False, False, False],\n",
      "       [ True, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False]])>), <tf.Tensor: id=35053, shape=(20, 512), dtype=int32, numpy=\n",
      "array([[   0,    0,    0, ...,    0,    0,    0],\n",
      "       [   0,    0,    0, ...,    0,    0,    0],\n",
      "       [   0, 3377,    0, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [6206,    0,    0, ...,    0,    0,    0],\n",
      "       [ 387,    0,    0, ...,    0,    0,    0],\n",
      "       [   0,    0,    0, ...,    0,    0,    0]], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(language_model_path)\n",
    "\n",
    "def sp_encode(line_tensor):\n",
    "    encoded_array = np.asarray(\n",
    "        sp.SampleEncodeAsIds(line_tensor.numpy(), nbest_size=-1, alpha=0.5))\n",
    "\n",
    "    # If the protein sequence is too long, take a random slice.\n",
    "    if len(encoded_array) > max_sequence_length:\n",
    "        offset = np.random.randint(\n",
    "            low=0, high=len(encoded_array) - max_sequence_length + 1)\n",
    "        encoded_array = encoded_array[offset:(offset + max_sequence_length)]\n",
    "\n",
    "    return encoded_array\n",
    "\n",
    "def sp_decode(line_tensor):\n",
    "    return sp.DecodeIds(line_tensor.numpy().tolist())\n",
    "\n",
    "def sp_encode_tf(line_tensor):\n",
    "    return tf.py_function(sp_encode, inp=[line_tensor], Tout=[tf.int32,])\n",
    "\n",
    "def mask_input(input_tensor):\n",
    "    \"\"\" Randomly mask the input tensor according to the formula perscribed by BERT. \n",
    "    Randomly masks 15% of input tokens, with 80% recieving the [MASK] token,\n",
    "    10% randomized, 10% left unchanged. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    masked_tensor: (batch_size, seq_length) \n",
    "        Tensor with masked values\n",
    "    input_tensor: (batch_size, seq_length)\n",
    "        Original input tensor (true values)\n",
    "    input_mask: (batch_size, seq_length)\n",
    "        Boolean mask that selects the desired inputs.    \n",
    "    \"\"\"\n",
    "\n",
    "    mask_score = tf.random.uniform(input_tensor.shape, maxval=1, dtype=tf.float32)\n",
    "    input_mask = mask_score < .15\n",
    "\n",
    "    # Mask with [MASK] token 80% of the time\n",
    "    mask_mask = mask_score <= 0.15 * 0.8 \n",
    "\n",
    "    # Mask with random token 10% of the time\n",
    "    mask_random = (mask_score >= 0.15 * 0.9) & input_mask\n",
    "\n",
    "    # Tensors to replace with where input is masked or randomized\n",
    "    mask_value_tensor = tf.ones(input_tensor.shape, dtype=tf.int32) * mask_index\n",
    "    random_value_tensor = tf.random.uniform(\n",
    "        input_tensor.shape, minval=vocab_start, maxval=vocab_size, dtype=tf.int32)\n",
    "    pad_value_tensor = tf.zeros(input_tensor.shape, dtype=tf.int32)\n",
    "\n",
    "    # Use the replacements to mask the input tensor\n",
    "    masked_tensor = tf.where(mask_mask, mask_value_tensor, input_tensor)\n",
    "    masked_tensor = tf.where(mask_random, random_value_tensor, masked_tensor)\n",
    "\n",
    "    # Set true values to zero (pad value) where not masked\n",
    "    true_tensor = tf.where(input_mask, input_tensor, pad_value_tensor)\n",
    "\n",
    "    return masked_tensor, input_mask, true_tensor\n",
    "\n",
    "def mask_input_tf(input_tensor):\n",
    "    a, b, c = tf.py_function(mask_input, inp=[input_tensor], Tout=[tf.int32, tf.bool, tf.int32])\n",
    "    return (a, b), c\n",
    "\n",
    "\n",
    "valid_data = tf.data.TextLineDataset(sequence_path)\n",
    "\n",
    "encoded_data = valid_data\\\n",
    "    .map(sp_encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "    .map(mask_input_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "# This argument controls whether to fix the size of the sequences\n",
    "tf_seq_len = -1 if not fix_sequence_length else max_sequence_length\n",
    "\n",
    "encoded_data = encoded_data\\\n",
    "    .shuffle(buffer_size=buffer_size)\\\n",
    "    .padded_batch(batch_size, padded_shapes=(([tf_seq_len], [tf_seq_len]), [tf_seq_len]))\n",
    "\n",
    "print(next(iter(encoded_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked_seqs, input_mask), true_values = next(iter(encoded_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT layers development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.layers import (PositionEmbedding, Attention, Transformer, TokenEmbedding, Bias,\n",
    "                         gelu, masked_sparse_cross_entropy_loss, BERTLearningRateScheduler)\n",
    "\n",
    "MAX_ENCODED_LENGTH = 512\n",
    "\n",
    "embedding = PositionEmbedding(MAX_ENCODED_LENGTH + 1, MAX_ENCODED_LENGTH, mask_zero=True)\n",
    "assert np.all(embedding(masked_seqs)[0, 2, :] == embedding(masked_seqs)[5, 2, :])\n",
    "\n",
    "inputs = embedding(masked_seqs)\n",
    "out = Attention(8, name='test')(inputs)\n",
    "out.shape\n",
    "\n",
    "transformer = Transformer(8, 0.1, name='test')\n",
    "out = transformer(inputs)\n",
    "out2 = transformer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class Bias(layers.Layer):\n",
    "    \"\"\" Final bias layer added to logits prior to softmax scoring. This layer\n",
    "    also clears the _keras_mask attribute from the Transformers in order to\n",
    "    allow the custom loss function to work properly. \"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bias = self.add_weight(name='classifier_bias',\n",
    "                                    dtype=K.floatx(),\n",
    "                                    shape=[input_shape[0][-1]],\n",
    "                                    initializer=tf.zeros_initializer())\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        logits = tf.nn.bias_add(inputs[0], self.bias)\n",
    "        return logits\n",
    "        \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_embedding_5 (TokenEmbeddi multiple             128000      input_15[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "position_embedding_6 (PositionE (None, 512, 4)       2052        input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 512, 4)       0           token_embedding_5[0][0]          \n",
      "                                                                 position_embedding_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 512, 16)      80          add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "transformer_5 (Transformer)     (None, 512, 16)      1424        dense_12[0][0]                   \n",
      "                                                                 transformer_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 512, 4)       68          transformer_5[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bias_5 (Bias)                   (None, 512, 32000)   32000       token_embedding_5[1][0]          \n",
      "                                                                 input_16[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 163,624\n",
      "Trainable params: 163,624\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "from bert.layers import initializer\n",
    "\n",
    "embedding_dimension = 4\n",
    "vocab_size = 32000\n",
    "model_dimension = 16\n",
    "num_transformer_layers = 2\n",
    "\n",
    "\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "# with mirrored_strategy.scope():\n",
    "\n",
    "inputs = layers.Input(shape=(MAX_ENCODED_LENGTH,), dtype=tf.int32, batch_size=None)\n",
    "input_mask = layers.Input(shape=(MAX_ENCODED_LENGTH,), dtype=tf.bool, batch_size=None)\n",
    "\n",
    "token_embedding_layer = TokenEmbedding(\n",
    "    vocab_size, embedding_dimension, embeddings_initializer=initializer(), mask_zero=True)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "position_embeddings = PositionEmbedding(\n",
    "    MAX_ENCODED_LENGTH + 1, embedding_dimension, embeddings_initializer=initializer(),\n",
    "    mask_zero=True)(inputs)\n",
    "\n",
    "embeddings = layers.Add()([token_embeddings, position_embeddings])\n",
    "embeddings = layers.Dense(model_dimension)(embeddings)\n",
    "\n",
    "transformer = Transformer(4)\n",
    "for i in range(num_transformer_layers):\n",
    "    embeddings = transformer(embeddings)\n",
    "\n",
    "out = layers.Dense(embedding_dimension, activation=gelu, kernel_initializer=initializer())(embeddings)\n",
    "out = token_embedding_layer(out, transpose=True)\n",
    "out = Bias()([out, input_mask])\n",
    "\n",
    "model = tf.keras.Model([inputs, input_mask], [out], name='model')\n",
    "model.summary()\n",
    "\n",
    "true_labels = layers.Input(shape=(None,), dtype=tf.int32, batch_size=None)\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    target_tensors=true_labels,\n",
    "    optimizer=tfa.optimizers.AdamW(weight_decay=0.01, learning_rate=1E-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pstjohn/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 23s 453ms/step - loss: 0.4449 - sparse_categorical_accuracy: 0.0130\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 0.4448 - sparse_categorical_accuracy: 0.0138 9s - loss: 0.4738 \n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 7s 144ms/step - loss: 0.4374 - sparse_categorical_accuracy: 0.0163 2s - loss: 0.4446 - sparse_categorical\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 7s 144ms/step - loss: 0.4321 - sparse_categorical_accuracy: 0.0164 4s - loss: 0.4492 - sparse_categorical_a - ETA: 1s - loss: 0.4348 - sparse_categorical_accur\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 0.4350 - sparse_categorical_accuracy: 0.0199 0s - loss: 0.4361 - sparse_categorical_accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d956c4b10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(encoded_data, epochs=5,\n",
    "#              callbacks=[BERTLearningRateScheduler(initial_learning_rate=1E-3)], \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    'test_model.h5',\n",
    "    custom_objects={\n",
    "        'PositionEmbedding': PositionEmbedding,\n",
    "        'TokenEmbedding': TokenEmbedding,\n",
    "        'Attention': Attention,\n",
    "        'Transformer': Transformer,\n",
    "        'Bias': Bias,\n",
    "        'gelu': gelu,\n",
    "    }, compile=False)\n",
    "\n",
    "true_labels = layers.Input(shape=(None,), dtype=tf.int32, batch_size=None)\n",
    "model.compile(loss=masked_sparse_cross_entropy_loss, target_tensors=true_labels,\n",
    "              optimizer=tfa.optimizers.AdamW(weight_decay=0.01, learning_rate=1E-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_data = valid_data.map(sp_encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE).padded_batch(60, padded_shapes=([512],))\n",
    "# eval_encoded = next(iter(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predict = model.predict(encoded_data.take(3), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predict.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
