{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='talk', style='ticks',\n",
    "        color_codes=True, rc={'legend.frameon': False})\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.dataset import create_masked_input_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = create_masked_input_dataset(\n",
    "    language_model_path='uniparc_5M.model',\n",
    "    sequence_path='sequences_train_subset.txt',\n",
    "    max_sequence_length=512,\n",
    "    batch_size=20,\n",
    "    buffer_size=1024,\n",
    "    vocab_size=32000,\n",
    "    mask_index=4,\n",
    "    vocab_start=5,\n",
    "    fix_sequence_length=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=322742, shape=(20, 512), dtype=int32, numpy=\n",
      "array([[  351,     4, 24101, ...,     0,     0,     0],\n",
      "       [   23,  4362,   320, ...,     0,     0,     0],\n",
      "       [    4,   102,     4, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [   23,     9,    10, ...,     0,     0,     0],\n",
      "       [   23,  3808,    11, ...,     0,     0,     0],\n",
      "       [   23,    39,   909, ...,     0,     0,     0]], dtype=int32)>, <tf.Tensor: id=322743, shape=(20, 512), dtype=int32, numpy=\n",
      "array([[   0,  162,  966, ...,    0,    0,    0],\n",
      "       [   0,    0,    0, ...,    0,    0,    0],\n",
      "       [4963,    0,  152, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   0,    0,    0, ...,    0,    0,    0],\n",
      "       [   0,    0,   11, ...,    0,    0,    0],\n",
      "       [   0,    0,    0, ...,    0,    0,    0]], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(encoded_data)))\n",
    "masked_seqs, true_values = next(iter(encoded_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT layers development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.layers import (PositionEmbedding, Attention, Transformer, TokenEmbedding, Bias,\n",
    "                         gelu, masked_sparse_cross_entropy_loss, BERTLearningRateScheduler)\n",
    "\n",
    "MAX_ENCODED_LENGTH = 512\n",
    "\n",
    "embedding = PositionEmbedding(MAX_ENCODED_LENGTH + 1, MAX_ENCODED_LENGTH, mask_zero=True)\n",
    "assert np.all(embedding(masked_seqs)[0, 2, :] == embedding(masked_seqs)[5, 2, :])\n",
    "\n",
    "inputs = embedding(masked_seqs)\n",
    "out = Attention(8, name='test')(inputs)\n",
    "out.shape\n",
    "\n",
    "transformer = Transformer(8, 0.1, name='test')\n",
    "out = transformer(inputs)\n",
    "out2 = transformer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_embedding_2 (TokenEmbeddi multiple             128000      input_5[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "position_embedding_4 (PositionE (None, 512, 4)       2052        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 512, 4)       0           token_embedding_2[0][0]          \n",
      "                                                                 position_embedding_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512, 16)      80          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "transformer_2 (Transformer)     (None, 512, 16)      1424        dense_8[0][0]                    \n",
      "                                                                 transformer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512, 4)       68          transformer_2[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bias_2 (Bias)                   (None, 512, 32000)   32000       token_embedding_2[1][0]          \n",
      "==================================================================================================\n",
      "Total params: 163,624\n",
      "Trainable params: 163,624\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-dd4e97785121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     model.compile(loss=masked_sparse_cross_entropy_loss, target_tensors=true_labels,\n\u001b[0;32m---> 40\u001b[0;31m                   optimizer=tfa.optimizers.AdamW(weight_decay=0.01, learning_rate=1E-3))\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     model.fit(encoded_data, epochs=5,\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m                                                              \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                                                              \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                                                              weighted_metrics)\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;31m# We've disabled automatic dependency tracking for this method, but do want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;31m# to add a checkpoint dependency on the optimizer if it's trackable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_validate_compile_param_for_distribution_strategy\u001b[0;34m(self, run_eagerly, sample_weight_mode, target_tensors, weighted_metrics)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         raise NotImplementedError('weighted_metrics is not supported with '\n\u001b[1;32m   1506\u001b[0m                                   'tf.distribute.Strategy.')\n\u001b[0;32m-> 1507\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1508\u001b[0m         raise ValueError('target_tensors is not supported with '\n\u001b[1;32m   1509\u001b[0m                          'tf.distribute.Strategy.')\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \"\"\"\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_bool_casting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m       \u001b[0;31m# Default: V1-style Graph execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_in_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using a `tf.Tensor` as a Python `bool`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_in_graph_mode\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    521\u001b[0m     raise errors.OperatorNotAllowedInGraphError(\n\u001b[1;32m    522\u001b[0m         \u001b[0;34m\"{} is not allowed in Graph execution. Use Eager execution or decorate\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \" this function with @tf.function.\".format(task))\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "from bert.layers import initializer\n",
    "\n",
    "embedding_dimension = 4\n",
    "vocab_size = 32000\n",
    "model_dimension = 16\n",
    "num_transformer_layers = 2\n",
    "\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "\n",
    "    inputs = layers.Input(shape=(MAX_ENCODED_LENGTH,), dtype=tf.int32, batch_size=None)\n",
    "\n",
    "    token_embedding_layer = TokenEmbedding(\n",
    "        vocab_size, embedding_dimension, embeddings_initializer=initializer(), mask_zero=True)\n",
    "    token_embeddings = token_embedding_layer(inputs)\n",
    "    position_embeddings = PositionEmbedding(\n",
    "        MAX_ENCODED_LENGTH + 1, embedding_dimension, embeddings_initializer=initializer(),\n",
    "        mask_zero=True)(inputs)\n",
    "\n",
    "    embeddings = layers.Add()([token_embeddings, position_embeddings])\n",
    "    embeddings = layers.Dense(model_dimension)(embeddings)\n",
    "\n",
    "    transformer = Transformer(4)\n",
    "    for i in range(num_transformer_layers):\n",
    "        embeddings = transformer(embeddings)\n",
    "\n",
    "    out = layers.Dense(embedding_dimension, activation=gelu, kernel_initializer=initializer())(embeddings)\n",
    "    out = token_embedding_layer(out, transpose=True)\n",
    "    out = Bias()(out)\n",
    "\n",
    "    model = tf.keras.Model([inputs], [out], name='model')\n",
    "    model.summary()\n",
    "    \n",
    "    true_labels = layers.Input(shape=(None,), dtype=tf.int32, batch_size=None)\n",
    "    model.compile(loss=masked_sparse_cross_entropy_loss, target_tensors=true_labels,\n",
    "                  optimizer=tfa.optimizers.AdamW(weight_decay=0.01, learning_rate=1E-3))\n",
    "\n",
    "    model.fit(encoded_data, epochs=5,\n",
    "#              callbacks=[BERTLearningRateScheduler(initial_learning_rate=1E-3)], \n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    'test_model.h5',\n",
    "    custom_objects={\n",
    "        'PositionEmbedding': PositionEmbedding,\n",
    "        'TokenEmbedding': TokenEmbedding,\n",
    "        'Attention': Attention,\n",
    "        'Transformer': Transformer,\n",
    "        'Bias': Bias,\n",
    "        'gelu': gelu,\n",
    "    }, compile=False)\n",
    "\n",
    "true_labels = layers.Input(shape=(None,), dtype=tf.int32, batch_size=None)\n",
    "model.compile(loss=masked_sparse_cross_entropy_loss, target_tensors=true_labels,\n",
    "              optimizer=tfa.optimizers.AdamW(weight_decay=0.01, learning_rate=1E-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_data = valid_data.map(sp_encode_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE).padded_batch(60, padded_shapes=([512],))\n",
    "# eval_encoded = next(iter(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3/Unknown - 6s 2s/step"
     ]
    }
   ],
   "source": [
    "bert_predict = model.predict(encoded_data.take(3), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 512, 32000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_predict.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
