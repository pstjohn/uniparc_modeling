{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='talk', style='ticks',\n",
    "        color_codes=True, rc={'legend.frameon': False})\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 512), (None, 512)), (None, 512)), types: ((tf.int32, tf.bool), tf.int32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from bert.dataset import create_masked_input_dataset\n",
    "from bert.layers import (PositionEmbedding, Attention, Transformer, TokenEmbedding, Bias,\n",
    "                         gelu, masked_sparse_cross_entropy_loss, BERTLearningRateScheduler,\n",
    "                         initializer)\n",
    "\n",
    "training_data = create_masked_input_dataset(\n",
    "    language_model_path='uniparc_10M.model',\n",
    "    sequence_path='/projects/bpms/pstjohn/uniparc/sequences_train.txt',\n",
    "    max_sequence_length=512,\n",
    "    batch_size=20,\n",
    "    buffer_size=1024,\n",
    "    vocab_size=32000,\n",
    "    mask_index=4,\n",
    "    vocab_start=5,\n",
    "    fix_sequence_length=True)\n",
    "\n",
    "training_data.repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_data = create_masked_input_dataset(\n",
    "    language_model_path='uniparc_10M.model',\n",
    "    sequence_path='/projects/bpms/pstjohn/uniparc/sequences_valid.txt',\n",
    "    max_sequence_length=512,\n",
    "    batch_size=20,\n",
    "    buffer_size=1024,\n",
    "    vocab_size=32000,\n",
    "    mask_index=4,\n",
    "    vocab_start=5,\n",
    "    fix_sequence_length=True)\n",
    "\n",
    "valid_data.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_embedding_7 (TokenEmbeddi multiple             1024000     input_19[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "position_embedding_9 (PositionE (None, 512, 32)      16416       input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 512, 32)      0           token_embedding_7[0][0]          \n",
      "                                                                 position_embedding_9[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 512, 64)      2112        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "transformer_7 (Transformer)     (None, 512, 64)      21056       dense_18[0][0]                   \n",
      "                                                                 transformer_7[0][0]              \n",
      "                                                                 transformer_7[1][0]              \n",
      "                                                                 transformer_7[2][0]              \n",
      "                                                                 transformer_7[3][0]              \n",
      "                                                                 transformer_7[4][0]              \n",
      "                                                                 transformer_7[5][0]              \n",
      "                                                                 transformer_7[6][0]              \n",
      "                                                                 transformer_7[7][0]              \n",
      "                                                                 transformer_7[8][0]              \n",
      "                                                                 transformer_7[9][0]              \n",
      "                                                                 transformer_7[10][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 512, 32)      2080        transformer_7[11][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bias_7 (Bias)                   (None, 512, 32000)   32000       token_embedding_7[1][0]          \n",
      "                                                                 input_20[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,097,664\n",
      "Trainable params: 1,097,664\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 32000\n",
    "max_seq_len = 512\n",
    "embedding_dimension = 32\n",
    "model_dimension = 64\n",
    "num_attention_heads = model_dimension // 16\n",
    "num_transformer_layers = 12\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "# with mirrored_strategy.scope():\n",
    "\n",
    "inputs = layers.Input(shape=(max_seq_len,), dtype=tf.int32, batch_size=None)\n",
    "input_mask = layers.Input(shape=(max_seq_len,), dtype=tf.bool, batch_size=None)\n",
    "\n",
    "token_embedding_layer = TokenEmbedding(\n",
    "    vocab_size, embedding_dimension, embeddings_initializer=initializer(), mask_zero=True)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "position_embeddings = PositionEmbedding(\n",
    "    max_seq_len + 1, embedding_dimension, embeddings_initializer=initializer(),\n",
    "    mask_zero=True)(inputs)\n",
    "\n",
    "embeddings = layers.Add()([token_embeddings, position_embeddings])\n",
    "embeddings = layers.Dense(model_dimension)(embeddings)\n",
    "\n",
    "transformer = Transformer(num_attention_heads)\n",
    "for i in range(num_transformer_layers):\n",
    "    embeddings = transformer(embeddings)\n",
    "\n",
    "out = layers.Dense(embedding_dimension, activation=gelu, kernel_initializer=initializer())(embeddings)\n",
    "out = token_embedding_layer(out, transpose=True)\n",
    "out = Bias()([out, input_mask])\n",
    "\n",
    "model = tf.keras.Model([inputs, input_mask], [out], name='model')\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    optimizer=tfa.optimizers.AdamW(weight_decay=0.01, learning_rate=1E-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix),\n",
    "    BERTLearningRateScheduler(initial_learning_rate=1E-3, num_warmup_steps=1000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4375 - sparse_categorical_accuracy: 0.0069WARNING:tensorflow:From /home/pstjohn/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(training_data, steps_per_epoch=100, epochs=10, verbose=1,\n",
    "                    validation_data=valid_data, validation_steps=10,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
